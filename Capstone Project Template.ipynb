{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "    This project process the I94 Immigration Data, World Temperature Data, U.S. City Demographic Data, Airport Code datasets to understand the following.\n",
    "\n",
    "        1. Number of Immigrants arrival for given airport for a given month.\n",
    "        2. Different immigrants residential country average temperature \n",
    "        3. Immigrants on arrival state address average population etc...\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "  ##### Project Plan/Data Sets:\n",
    "    Planning to import immigration, temperature, demographics and airport code datasets provided by udacity to understand the number of immigrants arrival by the given airport for the given month and also using the temperature data sets to predict the immigrants country of citizenship average temperature data and also using demographics data to predict the average population in the state they are going to reside in.\n",
    "    \n",
    "   \n",
    "   ##### End Solution:\n",
    "       In this project airflow is used to build the data pipeline. For handling different datasets, I have used Spark to process the different data sets in memory and the processed output is written into Parquet format in S3. Redshift is used to load the data for validation and also further required analysis can be performed in Redshift database itself. \n",
    "   \n",
    "   ##### Different Tools :\n",
    "           1. Airflow \n",
    "           2. S3\n",
    "           3. Spark/EMR\n",
    "           4. Redshift\n",
    "    \n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "   ##### Data sets\n",
    "        1. I94 Immigration Data: This data comes from the US National Tourism and Trade Office. It includes U.S. immigrant details for the year 2016.               \n",
    "        \n",
    "        2. World Temperature Data: This dataset came from Kaggle. It includes the temperature data of different city from different countries.\n",
    "        \n",
    "        3. U.S. City Demographic Data: This data comes from OpenSoft. This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. \n",
    "        \n",
    "        4. Airport Code Table: This is a simple table of airport codes and corresponding cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\t\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('data/input/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore/Cleaning the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "##### Immigration Data:\n",
    "Following data fields having floating 0 are converted to int\n",
    "1. cicid \n",
    "2. i94mon\n",
    "3. i94yr\n",
    "4. i94cit\n",
    "5. i94res\n",
    "6. arrdate\n",
    "7. i94mode\n",
    "8. i94addr\n",
    "9. depdate\n",
    "10. i94bir\n",
    "11. i94visa\n",
    "12. biryear\n",
    "\n",
    "Also, below fields are converted to appropriate data types as documented below\n",
    "\n",
    "1. admnum converted to Double\n",
    "2. fltno is converted to String\n",
    "3. Visatype is converted to String.\n",
    "    \n",
    "i94mode field having null values are converted to 0.\n",
    "\n",
    "##### Airport Data:\n",
    "\n",
    "local_code/airport_code containing values are removed from the dataset to have a unique value set and also distinct local_code data is selected.\n",
    "\n",
    "trim is applied for all the below data fields.\n",
    "\n",
    "1. local_code\n",
    "2. iata_code\n",
    "3. name\n",
    "4. type\n",
    "5. iso_region\n",
    "6. iso_country\n",
    "7. municipality\n",
    "8. gps_code\n",
    "\n",
    "coordinates is converted to String\n",
    "\n",
    "#### Temperature Data:\n",
    "\n",
    "Temperature Data is merged with immigration data country code and the data is grouped by country to get the average temperature of the given country\n",
    "\n",
    "#### U.S Demographics Data:\n",
    "\n",
    "Race, count data fields are dropped to get the distinct records for a given city.\n",
    "\n",
    "Demographics data is grouped by state code to get the state average male, female and total population of the given state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "    Data Model:\n",
    "\n",
    "   <img src=\"star_schema_v1.png\">\n",
    "\n",
    "    Following data model is chosen to answer the project scope related questions.\n",
    "\n",
    "    1. In order to get the immigrants count by given airport and year need to combine the airport_dim, time_dim and immig_fact table we can query the total immigrants arrival for the given airport name for the given month.\n",
    "\n",
    "    2. In order to get different immigrants residential country average temperature we need to combine the immig_fact and country_temperature_dim\n",
    "\n",
    "    3. In order to get the immigrant arrival state address average population, need to combine immig_fact and demographics_dim by stateCode.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "    1. Create different etl for different data sets\n",
    "    2. Immigration etl breaks the data into following fact and dimension tables\n",
    "        1. immig_fact\n",
    "        2. time_dim for arrival date\n",
    "        3. visa_type_dim\n",
    "        4. travel_mode_dim\n",
    "    3. Airport etl retrieves all distinct airport code along with airport name etc..\n",
    "    4. Temperature etl retrieves all country average temperature and average temperature uncertainity data\n",
    "    5. Demographics etl retrieves state average male, female and total population \n",
    "    6. Different etl jobs loads the data into parquet format into an S3 bucket.\n",
    "    7. Data pipeline loads the S3 parquet files into the redshift database and all the data validations are performed in the redshift db."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immigration data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:3.0.0-s_2.12\").enableHiveSupport().getOrCreate()\n",
    "df_immig =spark.read.format('com.github.saurfang.sas.spark').load('data/input/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "\n",
    "df_immig.createOrReplaceTempView(\"immigration_data\")\n",
    "\n",
    "output_data = \"data/output/\"\n",
    "\n",
    "immig_fact = spark.sql(\"\"\" select \n",
    "                    CAST(cicid as INT) as cicid,\n",
    "                    CAST(i94mon as INT) as i94_month,\n",
    "                    trim(CAST(i94port as String)) as partition_port,\n",
    "                    trim(CAST(i94port as String)) as i94_port,\n",
    "                    CAST(i94cit as INT) as i94_citizen,\n",
    "                    CAST(i94res as INT) as i94_resident, \n",
    "                    CAST(arrdate as INT) as i94_arrival_date,\n",
    "                    CAST(i94mode as INT) as travel_mode_id,\n",
    "                    CAST(i94addr as String) as state_code,\n",
    "                    CAST(depdate as INT) as i94_departure_date,\n",
    "                    CAST(i94bir as INT) as i94_age,\n",
    "                    CAST(i94visa as INT) as visa_category_id,\n",
    "                    to_date(dtadfile,'yyyyMMdd') as i94_date_added,\n",
    "                    to_date(dtaddto, 'yyyyMMdd') as i94_date_admitted,\n",
    "                    occup as i94_occupation,\n",
    "                    entdepa as i94_arrival_flag,\n",
    "                    entdepd as i94_departure_flag,\n",
    "                    entdepu as i94_update_flag,\n",
    "                    matflag as i94_match_flag,\n",
    "                    CAST(biryear as INT) as i94_birth_year,\n",
    "                    gender as i94_gender,\n",
    "                    insnum as i94_insnum,\n",
    "                    airline as i94_airline,\n",
    "                    CAST(admnum as Double) as i94_admission_number,\n",
    "                    CAST(fltno as String) as i94_flight_number,\n",
    "                    CAST(visatype as String) as i94_visa_type\n",
    "                    from \n",
    "                    immigration_data\"\"\")\n",
    "\n",
    "output_data = \"data/output/\"\n",
    "\n",
    "immig_fact_updated = immig_fact.na.fill(0, subset=[\"travel_mode_id\"])\n",
    "\n",
    "immig_fact_out = output_data + \"immig_fact.parquet\"\n",
    "\n",
    "immig_fact_updated.write \\\n",
    "        .mode(\"append\") \\\n",
    "        .partitionBy(\"partition_port\", \"i94_month\") \\\n",
    "        .parquet(immig_fact_out)\n",
    "\n",
    "\n",
    "\n",
    "visa_type_dim = spark.sql (\"\"\" select \n",
    "                        distinct CAST(i94visa as INT) as visa_category_id,\n",
    "                        CASE(CAST(i94visa as INT)) WHEN 1 then 'Business' WHEN 2 THEN 'Pleasure' ELSE 'Student' END as visa_type \n",
    "                        from \n",
    "                        immigration_data\"\"\")\n",
    "    \n",
    "visa_type_dim_out = output_data + \"visa_type_dim.parquet\"\n",
    "\n",
    "visa_type_dim.write \\\n",
    "        .mode(\"append\") \\\n",
    "        .parquet(visa_type_dim_out)\n",
    "\n",
    "\n",
    "\n",
    "travel_mode_dim = spark.sql (\"\"\" select \n",
    "                            distinct CAST(i94mode as INT) as travel_mode_id,\n",
    "                            CASE(CAST(i94mode as INT)) WHEN 1 then 'Air' WHEN 3 THEN 'Land' WHEN 2 THEN 'Sea' WHEN 9 THEN 'Not reported' ELSE 'Undefined' END as travel_mode \n",
    "                            from \n",
    "                            immigration_data\"\"\")\n",
    "\n",
    "travel_mode_dim_updated = travel_mode_dim.na.fill(0, subset=[\"travel_mode_id\"])\n",
    "\n",
    "travel_mode_dim_out = output_data + \"travel_mode_dim.parquet\"\n",
    "\n",
    "travel_mode_dim_updated.write \\\n",
    "        .mode(\"append\") \\\n",
    "        .parquet(travel_mode_dim_out)\n",
    "    \n",
    "\n",
    "time_dim = spark.sql (\"\"\" select \n",
    "                            distinct CAST(arrdate as INT) as arrival_sas,\n",
    "                            date_add(to_date('1960-01-01', \"yyyy-MM-dd\"), CAST(arrdate as INT)) as arrival_date,\n",
    "                            dayofmonth(date_add(to_date('1960-01-01', \"yyyy-MM-dd\"), CAST(arrdate as INT))) as day,\n",
    "                            month(date_add(to_date('1960-01-01', \"yyyy-MM-dd\"), CAST(arrdate as INT))) as month,\n",
    "                            month(date_add(to_date('1960-01-01', \"yyyy-MM-dd\"), CAST(arrdate as INT))) as parition_month,\n",
    "                            year(date_add(to_date('1960-01-01', \"yyyy-MM-dd\"), CAST(arrdate as INT))) as year,\n",
    "                            date_format(date_add(to_date('1960-01-01', \"yyyy-MM-dd\"), CAST(arrdate as INT)),\"E\") as weekday\n",
    "                            from \n",
    "                            immigration_data\"\"\")\n",
    "    \n",
    "time_dim_out = output_data + \"time_dim.parquet\"\n",
    "\n",
    "time_dim.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"parition_month\") \\\n",
    "    .parquet(time_dim_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airport Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "input_file_path = \"data/input/airport-codes_csv.csv\"\n",
    "\n",
    "# read airport data file\n",
    "df_airport_data = spark.read.csv(input_file_path, inferSchema=True, header=True)\n",
    "    \n",
    "# create a airport_data view from the spark dataframe\n",
    "df_airport_data.createOrReplaceTempView(\"airport_data\")\n",
    "\n",
    "# extract columns for country dimensions table    \n",
    "airport_dim = spark.sql (\"\"\"select distinct trim(local_code) as airport_code,\n",
    "                        trim(local_code) as partition_airport_code,\n",
    "                        trim(iata_code) as iata_code,\n",
    "                        trim(name) as airport_name,\n",
    "                        trim(type) as airport_type,\n",
    "                        trim(iso_region) as region,\n",
    "                        trim(iso_country) as country_code,\n",
    "                        trim(municipality) as municipality,\n",
    "                        trim(gps_code) as gps_code,\n",
    "                        CAST(coordinates as String) as coordinates\n",
    "                        from \n",
    "                        airport_data \n",
    "                        where\n",
    "                        local_code is not null\n",
    "                        \"\"\")\n",
    "\n",
    "output_data = \"data/output/\"\n",
    "\n",
    "# write users table to parquet files\n",
    "output_file_path = output_data + \"airport_dim.parquet\"\n",
    "    \n",
    "airport_dim.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"partition_airport_code\") \\\n",
    "    .parquet(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographics Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "input_file_path = \"data/input/us-cities-demographics.csv\"\n",
    "\n",
    "demographics_schema = StructType([\n",
    "        StructField(\"city\",StringType()),\n",
    "        StructField(\"state\",StringType()),\n",
    "        StructField(\"medianAge\",DoubleType()),\n",
    "        StructField(\"malePopulation\",IntegerType()),\n",
    "        StructField(\"femalePopulation\",IntegerType()),\n",
    "        StructField(\"totalPopulation\",IntegerType()),\n",
    "        StructField(\"noOfVeterans\",IntegerType()),\n",
    "        StructField(\"foreignBorn\",IntegerType()),\n",
    "        StructField(\"averageHouseHoldSize\",DoubleType()),\n",
    "        StructField(\"stateCode\",StringType()),\n",
    "        StructField(\"race\",StringType()),\n",
    "        StructField(\"count\",IntegerType())\n",
    "    ])\n",
    "\n",
    "# read demographics data file\n",
    "df_demographics_data = spark.read.csv(input_file_path, sep=';', schema = demographics_schema, header=True)\n",
    "        \n",
    "# Removing race and count column from data frame and then distinct is called to remove duplicates\n",
    "df_demographics_drop = df_demographics_data.drop(\"race\",\"count\")\n",
    "df_demographics_distinct = df_demographics_drop.distinct()\n",
    "    \n",
    "# create a demography_data view from the spark dataframe\n",
    "df_demographics_distinct.createOrReplaceTempView(\"demography_data\")\n",
    "\n",
    "# extract columns for demographics dimensions table    \n",
    "demographics_dim = spark.sql(\"\"\"select \n",
    "                            distinct stateCode,\n",
    "                            avg(medianAge) as average_median_age,\n",
    "                            avg(malePopulation) as average_male_population,\n",
    "                            avg(femalePopulation) as average_female_population,\n",
    "                            avg(totalPopulation) as average_total_population,\n",
    "                            avg(noOfVeterans) as average_no_of_veterans,\n",
    "                            avg(foreignBorn) as average_foreign_born,\n",
    "                            avg(averageHouseHoldSize) as average_house_hold_size\n",
    "                            from demography_data\n",
    "                            group by stateCode\"\"\")\n",
    "\n",
    "output_data = \"data/output/\"\n",
    "        \n",
    "# write users table to parquet files\n",
    "output_file_path = output_data + \"demographics_dim.parquet\"\n",
    "        \n",
    "demographics_dim.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .parquet(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----------------------+-----------------------------------+\n",
      "|country_code|country    |avg_country_temperature|avg_country_temperature_uncertainty|\n",
      "+------------+-----------+-----------------------+-----------------------------------+\n",
      "|384         |Chad       |27.189829394812683     |0.8463801152737748                 |\n",
      "|158         |Russia     |3.3472679828735536     |1.3651891063002695                 |\n",
      "|693         |Paraguay   |22.784014312977117     |0.828861164122139                  |\n",
      "|216         |Yemen      |25.76840766445382      |0.9586333735666872                 |\n",
      "|391         |Senegal    |25.984176694490824     |0.7867987312186983                 |\n",
      "|130         |Sweden     |5.665518003790269      |1.7004475047378378                 |\n",
      "|603         |Guyana     |26.54984937439856      |0.648079403272378                  |\n",
      "|243         |Burma      |26.016839989290098     |0.8800770498218452                 |\n",
      "|260         |Philippines|26.51646246746498      |0.636398638529612                  |\n",
      "|372         |Eritrea    |24.001515877771144     |0.9285710005991603                 |\n",
      "|322         |Djibouti   |29.152790108564506     |0.9232490952955368                 |\n",
      "|273         |Malaysia   |26.434756624383954     |0.6052510081423906                 |\n",
      "|207         |Singapore  |26.523102826510677     |0.621054580896688                  |\n",
      "|264         |Turkey     |12.951888167466656     |1.275864494241464                  |\n",
      "|345         |Malawi     |21.34787202649805      |0.6111836796145732                 |\n",
      "|250         |Iraq       |19.884738137449162     |1.060728021691523                  |\n",
      "|112         |Germany    |8.482790790263838      |1.6147039376710948                 |\n",
      "|236         |Afghanistan|13.81649689626358      |0.9516393697027569                 |\n",
      "|253         |Jordan     |18.360980886539238     |0.9316055307035377                 |\n",
      "|376         |Rwanda     |19.077631823461093     |0.7327491289198604                 |\n",
      "+------------+-----------+-----------------------+-----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "temp_data = \"data/input/GlobalLandTemperaturesByCity_Kaggle.csv\"\n",
    "\n",
    "# read temperature data file\n",
    "df_temp_data = spark.read.csv(temp_data, inferSchema=True, header=True)\n",
    "        \n",
    "# create a temperature_data view from the spark dataframe\n",
    "df_temp_data.createOrReplaceTempView(\"temperature_data\")\n",
    "\n",
    "#Retrieving country data to join with immigration data\n",
    "country_data = \"data/input/country_code.csv\"\n",
    "\n",
    "df_country = spark.read.csv(country_data, inferSchema=True, header=True)\n",
    "\n",
    "df_country.createOrReplaceTempView(\"country_table\")\n",
    "\n",
    "# extract columns for country dimensions table    \n",
    "country_dim = spark.sql (\"\"\"select c.country_code as country_code,\n",
    "                    t.Country as country,\n",
    "                    t.avg_country_temperature as avg_country_temperature,\n",
    "                    t.avg_country_temperature_uncertainty as avg_country_temperature_uncertainty\n",
    "                    from \n",
    "                    country_table c\n",
    "                    left join\n",
    "                    (select \n",
    "                        avg(AverageTemperature) as avg_country_temperature,\n",
    "                        avg(AverageTemperatureUncertainty) as avg_country_temperature_uncertainty,\n",
    "                        Country \n",
    "                        from \n",
    "                        temperature_data \n",
    "                        group by Country ) t  \n",
    "                        on t.Country = c.country\n",
    "                    where t.country is not null\n",
    "                    \"\"\")\n",
    "\n",
    "country_dim.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "temp_data = \"data/input/GlobalLandTemperaturesByCity_Kaggle.csv\"\n",
    "\n",
    "# read temperature data file\n",
    "df_temp_data = spark.read.csv(temp_data, inferSchema=True, header=True)\n",
    "        \n",
    "# create a temperature_data view from the spark dataframe\n",
    "df_temp_data.createOrReplaceTempView(\"temperature_data\")\n",
    "\n",
    "#Retrieving country data to join with immigration data\n",
    "country_data = \"data/input/country_code.csv\"\n",
    "\n",
    "df_country = spark.read.csv(country_data, inferSchema=True, header=True)\n",
    "\n",
    "df_country.createOrReplaceTempView(\"country_table\")\n",
    "\n",
    "# extract columns for country dimensions table    \n",
    "country_dim = spark.sql (\"\"\"select c.country_code as country_code,\n",
    "                    t.Country as country,\n",
    "                    t.avg_country_temperature as avg_country_temperature,\n",
    "                    t.avg_country_temperature_uncertainty as avg_country_temperature_uncertainty\n",
    "                    from \n",
    "                    country_table c\n",
    "                    left join\n",
    "                    (select \n",
    "                        avg(AverageTemperature) as avg_country_temperature,\n",
    "                        avg(AverageTemperatureUncertainty) as avg_country_temperature_uncertainty,\n",
    "                        Country \n",
    "                        from \n",
    "                        temperature_data \n",
    "                        group by Country ) t  \n",
    "                        on t.Country = c.country\n",
    "                    where t.country is not null\n",
    "                    \"\"\")\n",
    "\n",
    "output_data = \"data/output/\"\n",
    "\n",
    "# write users table to parquet files\n",
    "country_dim_out = output_data + \"country_dim.parquet\"\n",
    "        \n",
    "country_dim.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .parquet(country_dim_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Data quality checks are performed using DataQualityOperator available in data_quality.py and the same is invoked via airflow after loading the data into redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "    Data dictionary is available in data_dictionary.xlsx file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    \n",
    "        1. Airflow is chosen to run data pipeline with all the error handling in place on airflow makes it the best tool for executing the pipeline.\n",
    "        2. Spark/EMR is chosen to run all the mentioned ETL's. Spark is chosen to handle all the data in memory for processing and written in a specific parquet files\n",
    "        3. Redshift is chosen to load the data and perform validation and all the required analysis can be done in redshift db itself.\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "      \n",
    "      Job can run once in a day to load all the immigration, any change in temperature, demographics and airport information\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "        \n",
    "       If data is increased by 100x, then we need to spin up a big spark cluster to run the data in memory and loads the data into parquet files. S3 and other technology choice will still remain intact for the bigger data needs.\n",
    "          \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "          \n",
    "        Monitoring the job execution time for the data and scheduling it appropriately before 7 am will load the data into redshift then the dashboard connecting to redhsift will have the latest data by 7 am everyday.\n",
    "          \n",
    " * The database needed to be accessed by 100+ people.\n",
    "         \n",
    "        Database access for all 100+ users can be given in redshift by creating different users and providing different permissions so that each user activity can be tracked in the redshift, providing a common user access to everyone will be harmful to track the activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
